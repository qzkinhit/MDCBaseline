## `DataClean` `v1.0`测试报告

### 测试标准与要求

#### 标准

- 应变量：运行时间(`t`)和准确率(`ac`)；
- 自变量：数据量(`n`)、搜索深度(`depth`)、分块情况(`block`)，编辑惩罚(`edit`)；

​	围绕**运行时间**和**准确率**为核心，讨论各个**自变量**的**影响权重**，同时给出**最优的自变量参数**如何。

### 自参考数据集:`Flight` 数据集

#### 自参考数据集的测试算法：

采用约束违反测试，测试经过不同深度的搜索后，得到的结果数据集中违反自定义约束的情况。得到的结果值越大，代表数据集中违反约束的情况多

#### 数据情况：

- 原始数据集为1000 * 8 的数据集，存在大量缺失值以及各种格式错误

- 约束构成：

  ​	6种模式约束（定义单元格的数据格式，以正则表达式表示），7种函数依赖（一对一函数依赖）

- 测试结果：

  ​	质量函数测试结果评分为：4829.28

- 测试评价：

  ​	原始数据集存在大量空缺值和违反模式约束的情况，因此结果评分很高

- 数据集的约束设置情况：

  ​	针对6个属性的日期模式约束，和正则表达式的约束，以及飞机航班号对于其他信息的一对一约束

#### 自变量：`数据量`

- 测试方法：对于原始数据量为1000的数据集，取不同数据量的数据集进行测试，获取不同数据量下的时间信息及自测表现评分

- 测试结果

   数据量- 评分

  | 数据量   | 20     | 100    | 500      | 1000    |
  | -------- | ------ | ------ | -------- | ------- |
  | 初始评分 | 145.74 | 497.25 | 2497.481 | 4829.28 |
  | 结果评分 | 0      | 14.31  | 96.79    | 187.77  |

  

  数据量- 时间（搜索深度为8）

  | 数据量  | 20    | 100   | 500    | 1000   |
  | ------- | ----- | ----- | ------ | ------ |
  | 时间(s) | 10.54 | 46.83 | 240.88 | 362.14 |

  

#### 自变量：`搜索深度(depth)`

- 测试方法： 对于数据量为1000的数据集，测试不同搜索深度为时的数据集时间信息及自测表现评分
- 测试结果

  1.搜索深度-评分：

  | 搜索深度 | 0（初始） | 2      | 5      | 8      | 10     |
  | -------- | --------- | ------ | ------ | ------ | ------ |
  | 评分     | 4829.28   | 426.34 | 230.47 | 187.77 | 187.77 |

    2.搜索深度-时间：

  | 搜素深度 | 2      | 5      | 8      | 10     |
  | -------- | ------ | ------ | ------ | ------ |
  | 时间     | 186.29 | 227.69 | 362.14 | 370.47 |

  

- 测试评价

  ​	针对一千条数据集以及13种约束，匹配出最优修复序列的时间约为362.14秒，搜索深度为8


#### 自变量：`分块情况（block）`

- 测试方法：由于本数据集数据结构中只有唯一的属性能作为分块属性，因此本系统利用自带聚合分块操作，测试按照约束分块与否对系统带来的性能变化

- 测试结果

  |             | 分块   | 不分块 |
  | ----------- | ------ | ------ |
  | 运行时间(s) | 386.47 | 726.58 |

  



### 全参考数据集：`Hospital`数据集

选择`Hospital`数据集，一种数据清洗领域常见的数据集，参考文献如下：

``` 
X. Chu, I. F. Ilyas, and P. Papotti. Holistic data cleaning: Putting violations into context. In ICDE, pages 458–469,2013.
M. Dallachiesa, A. Ebaid, A. Eldawy, A. Elmagarmid, I. F.Ilyas, M. Ouzzani, and N. Tang. Nadeef: a commodity data cleaning system. In SIGMOD, pages 541–552, 2013.
```



`Hospital`数据集具有1000条数据，19条属性，20条以上的相关条件依赖和模式匹配，具有比较好的代表性。同时其中有脏数据和清洁数据两部分，方便我们进行全参考式的分析。

#### 全参考数据集的测试算法：

采用pandas的compares函数，对比处理的数据和现有的数据情况，将不一致的数据分别在处理前后进行计算，若计算结果为m,n,则将
$$
p=\frac{m-n}{m}
$$
作为准确率的计算方法；同时记录不同参数下的处理时间。

#### 数据集的约束设置情况：

#### 自变量：`数据量(n)`

- 测试方法

  在depth=5,以`HospitalName`作为分块属性的条件下，设置n为50，100，200，500，1000,统计准确率和时间的变化情况。

- 测试结果

  ```
  t=[0.6521739130434783, 0.7, 0.6021505376344086, 0.5431818181818182, 0.42986247544204322]
  p=[36.8729031085968, 76.4940276145935, 151.04747772216797, 352.74730134010315, 707.4098641872406]
  ```

  绘图如下：

  <img src="n_p2.png"/></img>

  <img src="n_t2.png"/></img>

- 测试评价

  ​	可以看出，时间和数据量的关系约为线性关系；准确率随着数据量的增加而降低，这是由于搜索深度，需要随着数据量的增加而相应增加，否则搜索出的序列质量较低；同时由于缺乏算子进行更细致的判断，时间变得更长，准确率变得更低。

#### 自变量：`搜索深度（depth)`

- 测试方法

  在n=1000,以`HospitalName`作为分块属性的条件下，设置depth为5，7，9，11，13,统计准确率和时间的变化情况。

- 测试结果

  ```
  p=[0.447937131631, 0.488821218075, 0.5114341846758, 0.485265225933]
  t=[725.629695892334, 1021.0421087741852, 1215.7561395168304, 1493.8742818832397]
  ```

<img src="d_p2.png"/></img>

<img src="d_t2.png"/></img>

- 测试评价

  数据量为1000时，搜素深度在9附近准确率最高；时间随搜素深度大致线性增加

#### 自变量：`分块情况（block）`

- 测试方法

  在n=1000，depth=9的条件下，切换不同的属性，统计准确率和时间的变化情况。

- 测试结果

  各个属性之间的差异较大，总结来说以`HospitalName`作为分块属性的条件下,准确率和时间都最为优秀。

- 测试评价

  ​	若要找到合适的分块属性，至少需要两个条件，一个是保证这个分块属性基本不会有问题，二是保证这个属性在所有条件约束的左侧或者间接通过变换能在左侧；

  ​	前者保证了修复的准确率，因为如果包含错误的数据，该分块是无效的；后者保证了有效性，因为只有分块属性能完全或者非完全决定右侧属性才能保证能产生有效的修复建议。


